{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hu-prog/Deep-Learning-course/blob/master/KAN_GAF_based_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoyKH4WcJUHp",
        "outputId": "5977b935-de36-4345-efd7-cab9a6a52636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyts\n",
            "  Downloading pyts-0.13.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from pyts) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from pyts) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyts) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from pyts) (1.4.2)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.11/dist-packages (from pyts) (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.55.2->pyts) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.0->pyts) (3.5.0)\n",
            "Downloading pyts-0.13.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyts\n",
            "Successfully installed pyts-0.13.0\n",
            "Collecting pykan\n",
            "  Downloading pykan-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading pykan-0.2.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pykan\n",
            "Successfully installed pykan-0.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pyts\n",
        "!pip install pykan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IlhpCzc4_nCr"
      },
      "outputs": [],
      "source": [
        "from helper_code import *\n",
        "import numpy as np, scipy as sp, scipy.stats, os, sys, joblib\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pyts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41TGewrZBBT6",
        "outputId": "914baccb-4854-4435-f8ff-8aee51482d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlDtuUbq_nCv"
      },
      "source": [
        "### GAF for features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IpVI90wc_nCx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "from pyts.image import GramianAngularField  # Library for GAF transformation\n",
        "\n",
        "def preprocess_audio(recording, max_length_sec=5, sample_rate=4000):\n",
        "    \"\"\"\n",
        "    Preprocess audio to ensure a maximum length and consistent sampling rate.\n",
        "\n",
        "    Parameters:\n",
        "    - recording: np.ndarray\n",
        "        Input audio signal.\n",
        "    - max_length_sec: int\n",
        "        Maximum allowed length of the audio in seconds.\n",
        "    - sample_rate: int\n",
        "        Desired sampling rate for the audio.\n",
        "\n",
        "    Returns:\n",
        "    - processed_audio: np.ndarray\n",
        "        Preprocessed audio signal.\n",
        "    \"\"\"\n",
        "    # Ensure the recording is a floating-point numpy array\n",
        "    recording = np.asarray(recording, dtype=np.float32)\n",
        "\n",
        "    # Resample the audio if the sample rate is different\n",
        "    if hasattr(recording, 'shape') and len(recording.shape) > 1 and recording.shape[1] > 1:\n",
        "        recording = librosa.to_mono(recording)  # Convert to mono if stereo\n",
        "\n",
        "    # Resample the audio to the desired sample rate\n",
        "    recording_resampled = librosa.resample(recording, orig_sr=sample_rate, target_sr=4000)\n",
        "\n",
        "    # Calculate max length in samples\n",
        "    max_samples = int(max_length_sec * sample_rate)\n",
        "\n",
        "    # Trim or pad to max length\n",
        "    if len(recording_resampled) > max_samples:\n",
        "        recording_resampled = recording_resampled[:max_samples]  # Trim\n",
        "    else:\n",
        "        recording_resampled = np.pad(recording_resampled, (0, max_samples - len(recording_resampled)), mode='constant')  # Pad with zeros\n",
        "\n",
        "    return recording_resampled\n",
        "\n",
        "def gramian_angular_field_transform(recording, sample_rate=4000, max_length_sec=5, image_size=128, method='summation'):\n",
        "    \"\"\"\n",
        "    Compute Gramian Angular Field (GAF) transformation with a unified size.\n",
        "\n",
        "    Parameters:\n",
        "    - recording: np.ndarray\n",
        "        The audio signal as a 1D array.\n",
        "    - sample_rate: int\n",
        "        Sampling rate of the audio (default: 4000 Hz).\n",
        "    - max_length_sec: int\n",
        "        Maximum allowed length of the audio in seconds.\n",
        "    - image_size: int\n",
        "        The size of the output GAF image (image_size x image_size).\n",
        "    - method: str\n",
        "        GAF method to use, either 'summation' or 'difference' (default: 'summation').\n",
        "\n",
        "    Returns:\n",
        "    - gaf_image: np.ndarray\n",
        "        The GAF image representation of the audio signal.\n",
        "    \"\"\"\n",
        "    # Preprocess the audio\n",
        "    processed_audio = preprocess_audio(recording, max_length_sec=max_length_sec, sample_rate=sample_rate)\n",
        "\n",
        "    # Normalize the signal to the range [-1, 1] as required by GAF\n",
        "    processed_audio = 2 * (processed_audio - np.min(processed_audio)) / (np.max(processed_audio) - np.min(processed_audio)) - 1\n",
        "\n",
        "    # Initialize the GAF transformer\n",
        "    gaf = GramianAngularField(image_size=image_size, method=method)\n",
        "\n",
        "    # Compute the GAF\n",
        "    gaf_image = gaf.fit_transform(processed_audio.reshape(1, -1))[0]\n",
        "\n",
        "    return gaf_image.flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "al_uIz1z_nCz"
      },
      "outputs": [],
      "source": [
        "def get_features(data, recordings):\n",
        "    # Extract the age group and replace with the (approximate) number of months for the middle of the age group.\n",
        "    age_group = get_age(data)\n",
        "\n",
        "    if compare_strings(age_group, 'Neonate'):\n",
        "        age = 0.5\n",
        "    elif compare_strings(age_group, 'Infant'):\n",
        "        age = 6\n",
        "    elif compare_strings(age_group, 'Child'):\n",
        "        age = 6 * 12\n",
        "    elif compare_strings(age_group, 'Adolescent'):\n",
        "        age = 15 * 12\n",
        "    elif compare_strings(age_group, 'Young Adult'):\n",
        "        age = 20 * 12\n",
        "    else:\n",
        "        age = float('nan')\n",
        "\n",
        "\n",
        "    # Extract height and weight.\n",
        "    height = get_height(data)\n",
        "    weight = get_weight(data)\n",
        "\n",
        "    # Extract pregnancy status.\n",
        "    is_pregnant = get_pregnancy_status(data)\n",
        "\n",
        "    # Extract recording locations and data. Identify when a location is present, and compute the mean, variance, and skewness of\n",
        "    # each recording. If there are multiple recordings for one location, then extract features from the last recording.\n",
        "    locations = get_locations(data)\n",
        "\n",
        "    recording_locations = ['AV', 'MV', 'PV', 'TV', 'PhC']\n",
        "    num_recording_locations = len(recording_locations)\n",
        "    recording_features = np.zeros((num_recording_locations, 16384), dtype=float)   # for GAF for featrues\n",
        "    num_locations = len(locations)\n",
        "    num_recordings = len(recordings)\n",
        "    if num_locations==num_recordings:\n",
        "        for i in range(num_locations):\n",
        "            for j in range(num_recording_locations):\n",
        "                if compare_strings(locations[i], recording_locations[j]) and np.size(recordings[i])>0:\n",
        "\n",
        "                    # print(len(gramian_angular_field_transform(recordings[i])))\n",
        "                    recording_features[j]=gramian_angular_field_transform(recordings[i])\n",
        "    recording_features = recording_features.flatten()\n",
        "\n",
        "    features = np.hstack((recording_features))\n",
        "\n",
        "    return np.asarray(features, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "607pjX_x_nC2"
      },
      "outputs": [],
      "source": [
        "def save_challenge_model(model_folder, imputer, murmur_classes, murmur_classifier, outcome_classes, outcome_classifier):\n",
        "    d = {'imputer': imputer, 'murmur_classes': murmur_classes, 'murmur_classifier': murmur_classifier, 'outcome_classes': outcome_classes, 'outcome_classifier': outcome_classifier}\n",
        "    filename = os.path.join(model_folder, 'model.sav')\n",
        "    joblib.dump(d, filename, protocol=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5WGZxHs8_nC3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kxzh0LuXtM1Q"
      },
      "outputs": [],
      "source": [
        "def GAF_feature_dataset(data_folder, model_folder, verbose):\n",
        "    # Find data files.\n",
        "    if verbose >= 1:\n",
        "        print('Finding data files...')\n",
        "\n",
        "    # Find the patient data files.\n",
        "    patient_files = find_patient_files(data_folder)\n",
        "    num_patient_files = len(patient_files)\n",
        "\n",
        "    if num_patient_files==0:\n",
        "        raise Exception('No data was provided.')\n",
        "\n",
        "    # Create a folder for the model if it does not already exist.\n",
        "    os.makedirs(model_folder, exist_ok=True)\n",
        "\n",
        "    # Extract the features and labels.\n",
        "    if verbose >= 1:\n",
        "        print('Extracting features and labels from the Challenge data...')\n",
        "\n",
        "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
        "    num_murmur_classes = len(murmur_classes)\n",
        "    outcome_classes = ['Abnormal', 'Normal']\n",
        "    num_outcome_classes = len(outcome_classes)\n",
        "\n",
        "    features = list()\n",
        "    murmurs = list()\n",
        "    outcomes = list()\n",
        "\n",
        "    for i in range(num_patient_files):\n",
        "        if verbose >= 2:\n",
        "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
        "\n",
        "        # Load the current patient data and recordings.\n",
        "        current_patient_data = load_patient_data(patient_files[i])\n",
        "        current_recordings = load_recordings(data_folder, current_patient_data)\n",
        "\n",
        "        # Extract features.\n",
        "        current_features = get_features(current_patient_data, current_recordings)\n",
        "        features.append(current_features)\n",
        "\n",
        "        # Extract labels and use one-GAF_based_Features.ipynbhot encoding.\n",
        "        current_murmur = np.zeros(num_murmur_classes, dtype=int)\n",
        "        murmur = get_murmur(current_patient_data)\n",
        "        if murmur in murmur_classes:\n",
        "            j = murmur_classes.index(murmur)\n",
        "            current_murmur[j] = 1\n",
        "        murmurs.append(current_murmur)\n",
        "\n",
        "        current_outcome = np.zeros(num_outcome_classes, dtype=int)\n",
        "        outcome = get_outcome(current_patient_data)\n",
        "        if outcome in outcome_classes:\n",
        "            j = outcome_classes.index(outcome)\n",
        "            current_outcome[j] = 1\n",
        "        outcomes.append(current_outcome)\n",
        "\n",
        "    features = np.vstack(features)\n",
        "    murmurs = np.vstack(murmurs)\n",
        "    outcomes = np.vstack(outcomes)\n",
        "\n",
        "    # Train the model.\n",
        "    # if verbose >= 1:\n",
        "    #     print('Training model...')\n",
        "\n",
        "    # Define parameters for random forest classifier.\n",
        "    # n_estimators   = 123  # Number of trees in the forest.\n",
        "    # max_leaf_nodes = 45   # Maximum number of leaf nodes in each tree.\n",
        "    # random_state   = 6789 # Random state; set for reproducibility.\n",
        "\n",
        "    # imputer = SimpleImputer().fit(features)\n",
        "    # features = imputer.transform(features)\n",
        "    # murmur_classifier = RandomForestClassifier(n_estimators=n_estimators, max_leaf_nodes=max_leaf_nodes, random_state=random_state).fit(features, murmurs)\n",
        "    # outcome_classifier = RandomForestClassifier(n_estimators=n_estimators, max_leaf_nodes=max_leaf_nodes, random_state=random_state).fit(features, outcomes)\n",
        "\n",
        "    # # Save the model.\n",
        "    # save_challenge_model(model_folder, imputer, murmur_classes, murmur_classifier, outcome_classes, outcome_classifier)\n",
        "\n",
        "    # if verbose >= 1:\n",
        "    #     print('Done.')\n",
        "    return features,murmurs,outcomes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD3_hxcK_nC4",
        "outputId": "df78d380-aa3b-43e6-8d2b-0aa177bdbdfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from kan import *\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import moviepy.video.io.ImageSequenceClip\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV_seEi8z7fG",
        "outputId": "9483dd7c-c125-475e-e560-7ff26faffba2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sjerG8M61oCt"
      },
      "outputs": [],
      "source": [
        "\n",
        "import umap.umap_ as umap  # Import UMAP\n",
        "def func_for_dim_reduc(feature_data):\n",
        "  # code for dim red using umap\n",
        "  reducer = umap.UMAP(n_components=900, random_state=42)  # Reduce to 2D for visualization\n",
        "  umap_features = reducer.fit_transform(feature_data)\n",
        "  updated_data = umap_features\n",
        "  print(\"updated_data shape: {}\".format(updated_data.shape))\n",
        "  return updated_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nv4uvw4V_nC6"
      },
      "outputs": [],
      "source": [
        "def load_iris_dataset(updated_features,labels_murmer):\n",
        "    # Load iris dataset\n",
        "    # iris = load_iris()\n",
        "    data = updated_features\n",
        "    target = labels_murmer\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
        "    target_tensor = torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "    # Split dataset into train and test sets\n",
        "    train_data, test_data, train_target, test_target = train_test_split(data_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create data loaders (optional, if you want to batch and shuffle the data)\n",
        "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_data, train_target), batch_size=1, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_data, test_target), batch_size=1, shuffle=False)\n",
        "\n",
        "    train_inputs = torch.empty(0, 900, device=device)\n",
        "    train_labels = torch.empty(0, 3,dtype=torch.long, device=device)\n",
        "    test_inputs = torch.empty(0, 900, device=device)\n",
        "    test_labels = torch.empty(0, 3,dtype=torch.long, device=device)\n",
        "\n",
        "    # Concatenate all data into a single tensor on the specified device\n",
        "    for data, labels in train_loader:\n",
        "        train_inputs = torch.cat((train_inputs, data.to(device)), dim=0)\n",
        "        train_labels = torch.cat((train_labels, labels.to(device)), dim=0)\n",
        "\n",
        "    for data, labels in test_loader:\n",
        "        test_inputs = torch.cat((test_inputs, data.to(device)), dim=0)\n",
        "        test_labels = torch.cat((test_labels, labels.to(device)), dim=0)\n",
        "\n",
        "    dataset = {}\n",
        "    dataset['train_input'] = train_inputs\n",
        "    dataset['test_input'] = test_inputs\n",
        "    dataset['train_label'] = train_labels\n",
        "    dataset['test_label'] = test_labels\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4yi5KCA3aDD",
        "outputId": "514a1a23-011c-4dc5-af27-55b420065ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding data files...\n",
            "Extracting features and labels from the Challenge data...\n"
          ]
        }
      ],
      "source": [
        "data_folder = \"/content/drive/MyDrive/Training_data\"\n",
        "model_folder = \"/content/drive/MyDrive/result_model_gaf_kan\"\n",
        "verbose=1\n",
        "\n",
        "features_data,labels_murmer,labels_outcome=GAF_feature_dataset(data_folder, model_folder, verbose)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNAla8MS5K1m",
        "outputId": "98cd18da-4335-40db-be05-6e588886f204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaN values: 0\n"
          ]
        }
      ],
      "source": [
        "features_data.shape\n",
        "import numpy as np\n",
        "\n",
        "print(\"Number of NaN values:\", np.isnan(features_data).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0l1kSN74GLU",
        "outputId": "b5d507a5-4b7b-4571-eab3-13f99400544f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated_data shape: (942, 900)\n"
          ]
        }
      ],
      "source": [
        "updated_features= func_for_dim_reduc(features_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "krQziy_s4LKd"
      },
      "outputs": [],
      "source": [
        "iris_dataset = load_iris_dataset(updated_features,labels_murmer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IG5i7X3_nC7",
        "outputId": "e0c0f4ed-a7a2-4fda-dc9e-572bebb8d66d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: torch.Size([753, 900])\n",
            "Train target shape: torch.Size([753, 3])\n",
            "Test data shape: torch.Size([189, 900])\n",
            "Test target shape: torch.Size([189, 3])\n",
            "====================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Train data shape: {}\".format(iris_dataset['train_input'].shape))\n",
        "print(\"Train target shape: {}\".format(iris_dataset['train_label'].shape))\n",
        "print(\"Test data shape: {}\".format(iris_dataset['test_input'].shape))\n",
        "print(\"Test target shape: {}\".format(iris_dataset['test_label'].shape))\n",
        "print(\"====================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VB_bIQuQlDpb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyOrL05q_nC8",
        "outputId": "d3a6b35c-43ee-4252-c604-e8ece4ce1a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint directory created: ./model\n",
            "saving model version 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4716, -1.0442, -0.1645],\n",
              "        [-0.4732, -1.0366, -0.1575],\n",
              "        [-0.4698, -1.0447, -0.1662],\n",
              "        ...,\n",
              "        [-0.4695, -1.0489, -0.1481],\n",
              "        [-0.4658, -1.0460, -0.1541],\n",
              "        [-0.5394, -0.9103, -0.3079]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Define a 2-layer KAN (input -> output)\n",
        "model = KAN(width=[900,100, 3], grid=10, k=3, seed=0, device=device)\n",
        "\n",
        "model(iris_dataset['train_input'])\n",
        "# model.plot(beta=100, scale=1, out_vars=['Present', 'Unknown', 'Absent'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vrtQHItyrbb2"
      },
      "outputs": [],
      "source": [
        "# Convert one-hot labels to class indices (shape [753])\n",
        "iris_dataset['train_label'] = torch.argmax(iris_dataset['train_label'], dim=1)\n",
        "iris_dataset['test_label'] = torch.argmax(iris_dataset['test_label'], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OWa-ygLVrxsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24422ea-4c3f-4aed-fc26-c7d242aa0457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([753])\n",
            "torch.int64\n"
          ]
        }
      ],
      "source": [
        "print(iris_dataset['train_label'].shape)  # Should be torch.Size([753])\n",
        "print(iris_dataset['train_label'].dtype)  # Should be torch.int64 (Long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "373GzRDM_nC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5909b66f-84ab-4b3b-ecb0-ad2da42659ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "| train_loss: 9.43e-01 | test_loss: 1.10e+02 | reg: 8.28e+02 | :   0%|      | 0/100 [00:11<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "def train_acc():\n",
        "    return torch.mean((torch.argmax(model(iris_dataset['train_input']), dim=1) == iris_dataset['train_label']).float())\n",
        "\n",
        "def test_acc():\n",
        "    return torch.mean((torch.argmax(model(iris_dataset['test_input']), dim=1) == iris_dataset['test_label']).float())\n",
        "image_folder = '/content/drive/MyDrive/result_model_gaf_kan'\n",
        "\n",
        "results = model.fit(iris_dataset, opt=\"Adam\", metrics=(train_acc, test_acc),\n",
        "                      loss_fn=torch.nn.CrossEntropyLoss(), steps=100, lamb=0.0001, lamb_entropy=10., save_fig=True, img_folder=image_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xyccOqi_nC9"
      },
      "outputs": [],
      "source": [
        "0ffgdt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xDTVqnR_nC-"
      },
      "source": [
        "### Getting True Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOpvHtzC_nC-"
      },
      "outputs": [],
      "source": [
        "def true_labels_provider(data_folder, model_folder, verbose):\n",
        "    # Find data files.\n",
        "    if verbose >= 1:\n",
        "        print('Finding data files...')\n",
        "\n",
        "    # Find the patient data files.\n",
        "    patient_files = find_patient_files(data_folder)\n",
        "    num_patient_files = len(patient_files)\n",
        "\n",
        "    if num_patient_files==0:\n",
        "        raise Exception('No data was provided.')\n",
        "\n",
        "    # Create a folder for the model if it does not already exist.\n",
        "    os.makedirs(model_folder, exist_ok=True)\n",
        "\n",
        "    # Extract the features and labels.\n",
        "    if verbose >= 1:\n",
        "        print('Extracting labels from the Challenge data...')\n",
        "\n",
        "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
        "    num_murmur_classes = len(murmur_classes)\n",
        "    outcome_classes = ['Abnormal', 'Normal']\n",
        "    num_outcome_classes = len(outcome_classes)\n",
        "\n",
        "    murmurs = list()\n",
        "    outcomes = list()\n",
        "\n",
        "    for i in range(num_patient_files):\n",
        "        if verbose >= 2:\n",
        "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
        "\n",
        "        # Load the current patient data and recordings.\n",
        "        current_patient_data = load_patient_data(patient_files[i])\n",
        "\n",
        "        # Extract labels and use one-hot encoding.\n",
        "        current_murmur = np.zeros(num_murmur_classes, dtype=int)\n",
        "        murmur = get_murmur(current_patient_data)\n",
        "        if murmur in murmur_classes:\n",
        "            j = murmur_classes.index(murmur)\n",
        "            current_murmur[j] = 1\n",
        "        murmurs.append(current_murmur)\n",
        "\n",
        "        current_outcome = np.zeros(num_outcome_classes, dtype=int)\n",
        "        outcome = get_outcome(current_patient_data)\n",
        "        if outcome in outcome_classes:\n",
        "            j = outcome_classes.index(outcome)\n",
        "            current_outcome[j] = 1\n",
        "        outcomes.append(current_outcome)\n",
        "\n",
        "    # features = np.vstack(features)\n",
        "    murmurs = np.vstack(murmurs)\n",
        "    outcomes = np.vstack(outcomes)\n",
        "    return murmurs, outcomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwEIbdWG_nC_"
      },
      "outputs": [],
      "source": [
        "data_folder = \"the-circor-digiscope-phonocardiogram-dataset-1.0.3/Training_data\"\n",
        "model_folder = \"result_model_gaf\"(data_folder, model_folder, verbose\n",
        "murmur_true, outcomes_true = true_labels_provider(data_folder, model_folder, verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8czK_Cw_nDA"
      },
      "outputs": [],
      "source": [
        "len(murmur_true)\n",
        "len(outcomes_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v56ybT6z_nDA"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# Do *not* edit this script. Changes will be discarded so that we can process the models consistently.\n",
        "\n",
        "# This file contains functions for training models for the 2022 Challenge. You can run it as follows:\n",
        "#\n",
        "#   python train_model.py data model\n",
        "#\n",
        "# where 'data' is a folder containing the Challenge data and 'model' is a folder for saving your model.\n",
        "\n",
        "import sys\n",
        "from helper_code import is_integer\n",
        "# from team_code import train_challenge_model\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "    # Parse the arguments.\n",
        "# if not (len(sys.argv) == 3 or len(sys.argv) == 4):\n",
        "#     raise Exception('Include the data and model folders as arguments, e.g., python train_model.py data model.')\n",
        "\n",
        "# Define the data and model foldes.\n",
        "data_folder = \"the-circor-digiscope-phonocardiogram-dataset-1.0.3/Training_data\"\n",
        "model_folder = \"result_model_gaf\"\n",
        "\n",
        "# Change the level of verbosity; helpful for debugging.\n",
        "# if len(sys.argv)==4 and is_integer(sys.argv[3]):\n",
        "#     verbose = int(sys.argv[3])\n",
        "# else:\n",
        "#     verbose = 1\n",
        "verbose=1\n",
        "\n",
        "train_challenge_model(data_folder, model_folder, verbose) ### Teams: Implement this function!!!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCq0kDRv_nDB"
      },
      "outputs": [],
      "source": [
        "def load_challenge_model(model_folder, verbose):\n",
        "    filename = os.path.join(model_folder, 'model.sav')\n",
        "    return joblib.load(filename)\n",
        "\n",
        "# Run your trained model. This function is *required*. You should edit this function to add your code, but do *not* change the\n",
        "# arguments of this function.\n",
        "def run_challenge_model(model, data, recordings, verbose):\n",
        "    imputer = model['imputer']\n",
        "    murmur_classes = model['murmur_classes']\n",
        "    murmur_classifier = model['murmur_classifier']\n",
        "    outcome_classes = model['outcome_classes']\n",
        "    outcome_classifier = model['outcome_classifier']\n",
        "\n",
        "    # Load features.\n",
        "    features = get_features(data, recordings)\n",
        "\n",
        "    # Impute missing data.\n",
        "    features = features.reshape(1, -1)\n",
        "    features = imputer.transform(features)\n",
        "\n",
        "    # Get classifier probabilities.\n",
        "    murmur_probabilities = murmur_classifier.predict_proba(features)\n",
        "    murmur_probabilities = np.asarray(murmur_probabilities, dtype=np.float32)[:, 0, 1]\n",
        "    outcome_probabilities = outcome_classifier.predict_proba(features)\n",
        "    outcome_probabilities = np.asarray(outcome_probabilities, dtype=np.float32)[:, 0, 1]\n",
        "\n",
        "    # Choose label with highest probability.\n",
        "    murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
        "    idx = np.argmax(murmur_probabilities)\n",
        "    murmur_labels[idx] = 1\n",
        "    outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
        "    idx = np.argmax(outcome_probabilities)\n",
        "    outcome_labels[idx] = 1\n",
        "\n",
        "    # Concatenate classes, labels, and probabilities.\n",
        "    classes = murmur_classes + outcome_classes\n",
        "    labels = np.concatenate((murmur_labels, outcome_labels))\n",
        "    probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
        "\n",
        "    return classes, labels, probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6trD6WLS_nDB"
      },
      "source": [
        "### inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB1FwDYB_nDB"
      },
      "outputs": [],
      "source": [
        "import numpy as np, os, sys\n",
        "from helper_code import *\n",
        "# from team_code import load_challenge_model, run_challenge_model\n",
        "\n",
        "# Run model.\n",
        "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
        "    # Load models.\n",
        "    if verbose >= 1:\n",
        "        print('Loading Challenge model...')\n",
        "\n",
        "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
        "\n",
        "    # Find the patient data files.\n",
        "    patient_files = find_patient_files(data_folder)\n",
        "    num_patient_files = len(patient_files)\n",
        "\n",
        "    if num_patient_files==0:\n",
        "        raise Exception('No data was provided.')\n",
        "\n",
        "    # Create a folder for the Challenge outputs if it does not already exist.\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Run the team's model on the Challenge data.\n",
        "    if verbose >= 1:\n",
        "        print('Running model on Challenge data...')\n",
        "    murmur_predicted=[]\n",
        "    outcome_predicted=[]\n",
        "    # Iterate over the patient files.\n",
        "    for i in range(num_patient_files):\n",
        "        if verbose >= 2:\n",
        "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
        "\n",
        "        patient_data = load_patient_data(patient_files[i])\n",
        "        recordings = load_recordings(data_folder, patient_data)\n",
        "\n",
        "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
        "        try:\n",
        "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
        "        except:\n",
        "            if allow_failures:\n",
        "                if verbose >= 2:\n",
        "                    print('... failed.')\n",
        "                classes, labels, probabilities = list(), list(), list()\n",
        "            else:\n",
        "                raise\n",
        "        murmur_predicted.append(labels[:3])\n",
        "        outcome_predicted.append(labels[3:])\n",
        "\n",
        "        # Save Challenge outputs.\n",
        "        head, tail = os.path.split(patient_files[i])\n",
        "        root, extension = os.path.splitext(tail)\n",
        "        output_file = os.path.join(output_folder, root + '.csv')\n",
        "        patient_id = get_patient_id(patient_data)\n",
        "        # save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
        "    if verbose >= 1:\n",
        "        print('Done.')\n",
        "    return murmur_predicted,outcome_predicted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kQ8NWwW_nDD"
      },
      "outputs": [],
      "source": [
        "# if __name__ == '__main__':\n",
        "    # Parse the arguments.\n",
        "    # if not (len(sys.argv) == 4 or len(sys.argv) == 5):\n",
        "    #     raise Exception('Include the model, data, and output folders as arguments, e.g., python run_model.py model data outputs.')\n",
        "\n",
        "    # Define the model, data, and output folders.\n",
        "model_folder = \"result_model_gaf\"\n",
        "data_folder = \"the-circor-digiscope-phonocardiogram-dataset-1.0.3/Training_data\"\n",
        "output_folder = \"result_model_gaf\"\n",
        "\n",
        "# Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
        "allow_failures = False\n",
        "\n",
        "# Change the level of verbosity; helpful for debugging.\n",
        "# if len(sys.argv)==5 and is_integer(sys.argv[4]):\n",
        "#     verbose = int(sys.argv[4])\n",
        "# else:\n",
        "#     verbose = 1\n",
        "verbose = 1\n",
        "\n",
        "murmur_predicted,outcome_predicted = run_model(model_folder, data_folder, output_folder, allow_failures, verbose)\n",
        "print(len(murmur_predicted))\n",
        "print(len(outcome_predicted))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wEe4lYw_nDE"
      },
      "source": [
        "### Murmur Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxYH6j-W_nDF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "# Compute accuracy and evaluation metrics\n",
        "accuracy = accuracy_score(murmur_true, murmur_predicted)\n",
        "precision = precision_score(murmur_true, murmur_predicted, average='weighted')\n",
        "recall = recall_score(murmur_true, murmur_predicted, average='weighted')\n",
        "f1 = f1_score(murmur_true, murmur_predicted, average='weighted')\n",
        "\n",
        "# Print results\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(murmur_true, murmur_predicted))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "y_true_single = np.argmax(murmur_true, axis=1)  # Convert to [0, 1, 2]\n",
        "y_predicted_single= np.argmax(murmur_predicted, axis=1)  # Convert to [0, 1, 2]\n",
        "print(confusion_matrix(y_true_single, y_predicted_single))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiTYwPBX_nDG"
      },
      "source": [
        "### outcome Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjLgS3Vi_nDG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "# Compute accuracy and evaluation metrics\n",
        "accuracy = accuracy_score(outcomes_true, outcome_predicted)\n",
        "precision = precision_score(outcomes_true, outcome_predicted, average='weighted')\n",
        "recall = recall_score(outcomes_true, outcome_predicted, average='weighted')\n",
        "f1 = f1_score(outcomes_true, outcome_predicted, average='weighted')\n",
        "s\n",
        "# Print results\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(outcomes_true, outcome_predicted))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "y_true_single = np.argmax(outcomes_true, axis=1)  # Convert to [0, 1, 2]\n",
        "y_predicted_single= np.argmax(outcome_predicted, axis=1)  # Convert to [0, 1, 2]\n",
        "print(confusion_matrix(y_true_single, y_predicted_single))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ7NBCmY_nDH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}